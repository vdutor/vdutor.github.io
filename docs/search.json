[
  {
    "objectID": "talks/index.html",
    "href": "talks/index.html",
    "title": "",
    "section": "",
    "text": "🎤 Talks\nA list of lectures or invited talks that I’ve given with links to slides or recording if available:\n2023\n\n\n\nReading group on Scalable Approaches to Self-Supervised Learning using Spectral Analysis in CBL in Cambridge. Notes\n\n\nTutorial on Score-based generative modelling and Introduction to ongoing research for SUMO at Ghent University. Slides (PDF).\n\n\n\n2022\n\n\n\nTea talk at CBL Cambridge on “How in-context learning of transformers can be cast as Bayesian inference”. Slides\n\n\nGenerative Models and Uncertainty Quantification (GeNU) in Copenhagen. Slides\n\n\nGaussian Process Summer School in Sheffield. Slides\n\n\nResearch Talk at CBL in Cambridge on Spherical Gaussian Processes. Slides\n\n\nReading group on Diffusion models in CBL in Cambridge. Slides\n\n\n\n2021\n\n\n\nSpotlight talk: probprog 2021\n\n\nAI for the study of Environmental Risks (AI4ER). Guest lecture on Gaussian process models and their applications. Slides\n\n\nStatML Oxford/Imperial College PhD program: Guest lecture on Variational Gaussian processes.\n\n\n\n2020\n\n\n\nICML 2020 on Spherical harmonic features in Gaussian processes. Slides\n\n\nAISTATS 2020 on Deep Convolutional Gaussian Processes Slides\n\n\nMicrosoft Research Cambridge: Research Spotlight.\n\n\nLancaster University: Workshop on Simulations for Climate Modelling."
  },
  {
    "objectID": "index.html#news",
    "href": "index.html#news",
    "title": "",
    "section": "News",
    "text": "News\n\n\n\nJun 1, 2023. Very happy to have three papers accepted at ICML 2023 on Neural Diffusion Processes, Memory-based Meta-Learning and Spherical-Orthogonal features.\n\n\nDec 8, 2022. Helped organizing a local NeurIPS meetup in Cambridge.\n\n\nSep 16, 2022. Was invited to give a talk at GeNU 2022 in Copenhagen on diffusion models.\n\n\nSep 14, 2022. Gave a lecture at the Gaussian process summer school in Sheffield on Spectral and Spherical Gaussian processes.\n\n\nSep 1, 2022. Start internship at DeepMind. I’ll be spending the fall in London working on technical analysis of transformer models.\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#research",
    "href": "index.html#research",
    "title": "",
    "section": "Research",
    "text": "Research\nHighlighted research papers. A complete list can be found on my Google scholar.\n\n\nNeural Diffusion Processes\nVincent Dutordoir, Alan Saul, Zoubin Ghahramani, Fergus Simpson\nInternational Conference on Machine Learning (ICML 2023)\nGenerative models that define a probabilistic model over functions via their finite marginals using probabilistic diffusion models.\n\n    \n        PDF\n    \n    \n        BibTeX\n    \n\n\n\nDeep neural networks as point estimates for deep Gaussian processes\nVincent Dutordoir, James Hensman, Mark van der Wilk, Carl Henrik Ek, Zoubin Ghahramani, Nicolas Durrande\nAdvances in Neural Information Processing Systems (NeurIPS 2021)\nShows the equivalence between the forward propogation of the mean in a Deep Gaussian process (GP) and a fully-connected neural network layer. Method can be used to better initialise deep GPs and understand the uncertainty in DNNs.\n\n    \n        PDF\n    \n    \n        BibTeX\n    \n\n\n\nSparse Gaussian Processes with Spherical Harmonic Features\nVincent Dutordoir, Nicolas Durrande, James Hensman\nInternation Conference of Machine Learning (ICML 2020)\nWe derive the Reproducing Kernel Hilbert space for spherical kernels allowing us to design linear time Gausian processes (GPs).\n\n    \n        PDF\n    \n    \n        BibTeX\n    \n\n\n\nNo matching items"
  },
  {
    "objectID": "notes/tf-rex.html",
    "href": "notes/tf-rex.html",
    "title": "TF-rex: Playing Google’s T-rex game with TensorFlow",
    "section": "",
    "text": "The goal of this project is to play Google’s offline T-rex Dino game using Deep Q-Learning. We use TensorFlow (TF) – hence the name TF-rex ;)\nAll source code and trained models can be found on GitHub.\nThe goal of this project is to play Google’s offline T-rex Dino game using Reinforcement Learning (RL). The RL algorithm is based on the Deep Q-Learning algorithm [1] and is implemented in TensorFlow (TF), hence the name TF-rex ;).\nGoogle’s offline game consists of a T-rex striving to dodge obstacles, such as cactuses and birds, and surviving as long as possible. The dino is able to perform three actions: “jumping”, “ducking” and “going forward”. You can try it yourself.\nIn this post, I’ll talk about the project’s implementation, the RL setup and algorithms and the training procedure. We assume that the reader is familiar with the basics of RL. If not, no worries! This blogpost from Arthur Juliani will quickly get you up to speed with some of the essentials."
  },
  {
    "objectID": "notes/tf-rex.html#overview",
    "href": "notes/tf-rex.html#overview",
    "title": "TF-rex: Playing Google’s T-rex game with TensorFlow",
    "section": "Overview",
    "text": "Overview\nWe want to create an “AI algorithm” that can play Google Chrome’s T-rex game. This project isn’t the first one to try this, but compared to existing projects [2, 3], we do two things differently. First, the AI is interacting in real-time with the real game, hosted in the browser. We are not using any sort of emulator or framework to slow down the game or frame rate. While this makes the framework genuinely more interesting and more directly applicable to other browser-based games, it requires some extra engineering. For example, we need to extract the game state from the browser and implement a duplex channel to allow for communication between the AI program and the browser. This channel is necessary to pass information such as actions and game states.\nSecondly, the AI algorithm is trained with images of the game state. This is in contrast to earlier attempts extracting useful features (e.g. distance to next obstacle and length of the next obstacle) to train the AI. In our opinion, this setup resembles a real-life learning environment much closer.\nThe codebase exists of two larger components: the javascript T-rex game and the AI python program. Both modules are given in the diagram below. They communicate through a bidirectional websocket using predefined commands.\n\n\n\nArchitecture\n\n\nThe diagram depicts the typical RL cycle [4]: an agent interacts with its environment in discrete time steps. At each time, the agent receives an observation, which typically includes the reward and state of the game. It then chooses an action from the set of available actions, which it sends to the environment. The environment moves to a new state by executing the action and the reward associated with the transition is determined. The goal of a reinforcement learning agent is to collect as much reward as possible. More on this below.\nIn the next section “Implementation” we’ll talk about the structure of the code and how we turned the in-browser game into an RL environment. In the “Reinforcement Learning” section the AI algorithm is discussed. In that section we abstract the fact that we are interacting with a real-life game and assume that we have an environment that provides us with the next state and associated reward, given a new action."
  },
  {
    "objectID": "notes/tf-rex.html#implementation",
    "href": "notes/tf-rex.html#implementation",
    "title": "TF-rex: Playing Google’s T-rex game with TensorFlow",
    "section": "Implementation",
    "text": "Implementation\n\nCreating the RL environment\nWe want to turn the javascript game, which is running inside the browser, into an RL environment. This means that we want to have a similar interface as, for example, OpenAI Gym environments. The Open AI interface looks as follows [5]\nob0 = env.reset() # sample environment state, return first observation\na0 = agent.act(ob0) # agent chooses first action\nob1, rew0, done0, info0 = env.step(a0) # environment returns observation,\n# reward, and boolean flag indicating if the episode is complete.\na1 = agent.act(ob1)\nob2, rew1, done1, info1 = env.step(a1)\n...\na99 = agent.act(o99)\nob100, rew99, done99, info2 = env.step(a99)\n# done99 == True => terminal\nTo achieve this, we need to conquer some hurdles. We need to… 1. implement a bidirectional communication channel between the browser/javascript code and the Python AI program, 2. perform actions, i.e. let the dino jump and duck. 3. extract the game state from the game and turn it into a parseable format. Let’s tackle these one by one.\nTo allow for bidirectional communication, we extent the game’s javascript code. We chose to use WebSocket objects as the transportation medium. When the page is done loading (i.e. onDocumentLoad event) the code will try to establish a connection with an entity running on 127.0.0.1:9090.\nfunction onDocumentLoad() {\n    runner = new Runner('.interstitial-wrapper');\n};\ndocument.addEventListener('DOMContentLoaded', onDocumentLoad);\nvar socket = new WebSocket(\"ws://127.0.0.1:9090\"); # will connect Python-side server\nThe only thing left to do is to create the listening entity on the Python side. In the project’s repo you can find the websocket_server.py file, which contains the implementation of a websocket server. When a Environment object is initialized, it will create such a websocket and order it to listen for connections on 127.0.0.1:9090. (Additionally, the socket will be placed in its own thread and communicate with the main thread using a multiprocessing.Queue.) As long the browser doesn’t establish a connection with this socket, the Python program will halt.\nclass Environment:\n    \"\"\"\n    Environment is responsible for the communication with the game, through the socket.\n    \"\"\"\n    def __init__(host, port): # host = 127.0.0.1 and port = 9090\n        self.queue = multiprocessing.Queue()\n        self.server = WebsocketServer(port, host=host)\n        thread = threading.Thread(target = self.server.run_forever)\n        thread.start()\n  ...\nAs long as both entities -the javascript code inside the browser and the Python WebSocket object- stay alive, we have a duplex communication channel. As a result, on both sides we have functions to send messages and we have callback-methods to process incoming messages. We will use this to solve the next hurdle: executing actions in the game, requested by the AI program.\nThe javascript snippet below shows the onmessage-callback in the javascript code, which is ran when a new message from the Python code comes in\nsocket.onmessage = function (event)\n{\n    var command = event.data;\n    var runner = new Runner();\n    console.log(command);\n\n    switch (command) {\n        case 'STATE':\n            runner.postState(socket);\n            break;\n        case 'START':\n            simulateKey(\"keydown\", 32); // space\n            setTimeout(function() {simulateKey(\"keyup\", 32);}, 1000);\n            break;\n        case 'REFRESH':\n            location.reload(true);\n            break;\n        case 'UP':\n            simulateKey(\"keydown\", 38); // arrow up\n            setTimeout(function() {simulateKey(\"keyup\", 38);}, 400);\n            break;\n        case 'DOWN':\n            simulateKey(\"keydown\", 40); // arrow down\n            setTimeout(function() {simulateKey(\"keyup\", 40);}, 400);\n            break;\n        default:\n    }\n};\nYou can see that we specified a simple communication-protocol, consisting of 5 commands: ‘STATE’, ‘START’, ‘REFRESH’, ‘UP’ and ‘DOWN’. The latter two, ‘UP’ and ‘DOWN’, are actions used in the game to control the dino. We execute the action by simulating a keypress on the keyboard. This is easily done in javascript using function simulateKey(type, keyCode) in combination with a setTimeout(). For example, when the Python AI program sends the message ‘UP’, the javascript code will first receive the message and subsequently simulate a press on the arrow-up key, which will cause the T-rex to jump.\nThe first three actions, ‘STATE’, ‘START’ and ‘REFRESH’, are controlling commands. The ‘STATE’ command will issue the javascript code to collect the current game state (i.e. the current image displaying the dino and the obstacles) and send it over the socket to the python side. The postState-function, ran when a ‘STATE’ message is received, looks like this\npostState: function (socket) {\n    console.log(\"in postState function\");\n    var canvas = document.getElementById('runner-id');\n    var dataUrl = canvas.toDataURL(\"image/png\");\n    var state = {\n        world: dataUrl,\n        crashed: this.crashed.toString()\n    }\n    socket.send(JSON.stringify(state))\n    }\nThe function will read the canvas - this is the PNG image you see on the screen - and parse it into a base64-encoded string. Next, a state-message is created with the base64-encoded image-string and a boolean indicating whether or not the dino is still alive. The state-message is parsed into JSON format and sent over the socket. On the python side, we parse the state-message as follows\ndata = json.loads(message)\nimage, crashed = data['world'], data['crashed']\n\n# remove data-info at the beginning of the image\nimage = re.sub('data:image/png;base64,', '', image)\n# convert image from base64 decoding to 2D numpy array\nimage = np.array(Image.open(BytesIO(base64.b64decode(image)))) # <-- tricky one\n# cast string boolean to python boolean\ncrashed = True if crashed in ['True', 'true'] else False\nThe most important line in the snippet above is the decoding of the base64 image-string into a 2D image matrix. All the functionality is provided by standard Python libraries, such as PIL (Python Image Library), JSON and base64. However, it took me some time to find the correct ones as weird behaviour occurs easily when passing images between different programming languages or even libraries.\nWe faced the three obstacles that were required to turn the in-browser game into an RL environment. The Environment class contains most of this logic and provides an easy interface to the game. The most important method is do_action() (similar to act() in Open AI Gyms)\nclass Environment:\n  def __init__():\n    # see above\n\n  def do_action(self, action):\n     \"\"\"\n     Performs an action and returns the next state, reward and crash status\n     \"\"\"\n     if action != Action.FORWARD:\n         # noting needs to be send when the action is going forward\n         self.server.send_message(self.game_client, self.actions[action])\n\n     time.sleep(0.1)\n\n     return self._get_state(action)\n\n def _get_state(self, action):\n     self.server.send_message(self.game_client, \"STATE\")\n     next_state, crashed = self.queue.get() # <-- halt while queue is empty (waiting for state-message)\n     reward = _calculate_reward(action, crashed)\n     return next_state, reward, crashed\nBesides the do_action() method, a Environment-object can also start() and refresh() the game. In a similar fashion as the Open AI Gym function reset().\n\n\nArchitecture\nThe diagram below shows the overall architecture of the project. It illustrates the division of the javascript code running inside the browser and the python code, communicating over a websocket. The classes WebSocket and Environment were discussed above. In the next section, we’ll focus on the remaining classes. They are responsible for the actual learning.\n\n\n\nArchitecture"
  },
  {
    "objectID": "notes/tf-rex.html#reinforcement-learning-rl",
    "href": "notes/tf-rex.html#reinforcement-learning-rl",
    "title": "TF-rex: Playing Google’s T-rex game with TensorFlow",
    "section": "Reinforcement Learning (RL)",
    "text": "Reinforcement Learning (RL)\nTo teach the dino how to dodge the approaching obstacles we chose a Deep Q-learning approach, proposed in [1] by DeepMind. We briefly discuss this algorithm from a theoretical point-of-view, and explain how we implemented it.\nThe main idea of Deep Q-learning is to use a (deep) parametric neural network to approximate the Q-function. The Q-function of a Markov decision process (MDP), often denoted by  gives the expected utility of taking a given action  in a given state  and following an optimal policy thereafter. In other words, the Q-function  is a function which takes as arguments an action and a state and returns the expected total future reward of the agent if it would execute the action  in state  and continue performing optimal actions. As we wish to maximize our reward in every state we typically execute the action which optimizes the total future reward, namely . In the literature, this is referred to as the agent’s policy.\nAn important property of the Q-function is that , where  is the state the agent ends up by performing action  in state ,  is the corresponding reward and  is the discount factor. This equation is referred to as the Bellman equation. We will later use it to train our neural network.\nIf we let the agent interact with the environment for a while, we end up with a collection of SARSA elements. SARSA stands for State, Action, Reward, State’ and Action’. It holds the current state of the agent S, the action the agent chooses A, the reward R the agent gets for choosing this action, the state S’ that the agent will now be in after taking that action, and finally the next action A’ the agent will choose in its new state. Taking every letter in the quintuple yields the word SARSA. This sequence is depicted in the figure below.\n\n\n\nRL loop\n\n\nIn the context of TF-rex is our action space  limited to three elements: “ducking”, “jumping” and “going forward”. This makes it relatively easy as we only need to deal with a small number of discrete actions. Handling continuous actions or a large space of discrete ones makes the learning typically much harder. The state space, on the other hand, is quite large as it consists out of four stacked images of size 80×80 (i.e. input dimensionality, ). In the next section we show how we create these input vectors.\n\nPreprocessing\nWe don’t directly use the images we receive from the javascript game as states. We need to preprocess them before using them as the inputs of the deep Q-learning algorithm. This accelerates the training as we eliminate noisy parts from the image. We also collect multiple images into a single state which serves as a kind of memory.\nThe image below shows the original version, i.e. the image collected and sent by the javascript game and received by the Environment. It is a grey-scale image and has dimensions 150×600. The highscore and current score are shown in the upper-right corner.\n\n\n\nOriginal image\n\n\nWe apply two preprocessing steps on this image.\n\nextract a region-of-interest (ROI)\nTo dodge the obstacles the left part of the image is clearly much more informative than the right side. Therefore, we select roi = image[:, 420], which drops 30% of the right-side pixels. This operation reduces the number of input pixels, as roi is 150×420 and removes the meta-information in the upper-right corner.\nremove harmless objects\nThe cloud, which you can see in the middle of the original image, doesn’t hurt the dino. The dino can touch it (i.e. have overlapping pixels) without dying. Harmless obstacles are easily filtered out, as they have a lighter colour than real obstacles, so we chose to remove them from the images using some straightforward masking techniques.\n\n\n\n\nROI and Remove clouds\n\n\nWe then apply a standard preprocessing step (inspired from the Atari games paper): resizing the image to 80×80 grid of pixels.\n\n\n\nSquaring\n\n\nFinally, we stack the last 4 frames in order to produce an 80×80×4 array which serves as a state for the Deep Q-Learning algorithm.\nAll these operations are performed by the Preprocessor class.\n\n\nLearning\nLike most RL algorithms, Deep Q-learning uses a SARSA observation to get an unbiased estimate of the error  where  denotes that the Q-function is built out of a neural net with parameters . Minimizing  w.r.t.  is used to optimize the neural network parameters.\nWhile this approach works in theory, in practice we see that that during the optimization the neural net tends to oscillate or diverge. See, for example, David Silver’s courses for an explanation why [6]. A couple of tricks are introduced to reduce this unwanted behaviour.\n1. Experience Replay\nInstead of using only a single SARSA observation in the error function  we use a batch of observations. This breaks correlations in the data, reduces the variance of the gradient estimator, and allows the network to learn from a more varied array of past experiences.\nFrom an implementation point of view this means that we need to store each observed SARSA element. At training time, we also need to be able to sample from this memory in order to get a new batch of experiences. We implemented a Memory class which does exactly this. We choose to use a FIFO-queue to store the SARSA elements.\nclass Memory:\n\n    def __init__(self, size):\n        self.size = size\n        self.mem = np.ndarray((size,5), dtype=object)\n        self.iter = 0\n        self.current_size = 0\n\n    def remember(self, state1, action, reward, state2, crashed):\n        self.mem[self.iter, :] = state1, action, reward, state2, crashed\n        self.iter = (self.iter + 1) % self.size\n        self.current_size = min(self.current_size + 1, self.size)\n\n    def sample(self, n):\n        n = min(self.current_size, n)\n        random_idx = random.sample(list(range(self.current_size)), n)\n        sample = self.mem[random_idx]\n        return (np.stack(sample[:, i], axis=0) for i in range(5))\n2. Target Network\nIn order to avoid oscillations, we will use a different network, called the target network, to estimate the Q-values during several epochs. The target network has fixed parameters, which reduces the variance and makes the learning more stable. We update the target network parameters with the values of our main network periodically. The loss is now calculated as  where  is our main network and  is our target network. Importantly, both networks have the same architecture but can have different values for the parameters. Periodically we need to update the target parameters with the newest values, i.e.  . By using this seperate network to compute the targets we get a more stable training procedure as we reduce the number of constantly shifting values in the loss function.\nMost of the learning logic is implemented in the DDQNAgent class. Below we show the method that computes the targets. We start by sampling for the experience memory and then calculate  for every SARSA element in the sample.\nclass DDQNAgent:\n\n    def replay(self):\n        states, actions, rewards, states_next, crashes = self.memory.sample(self.batch_size)\n        target = rewards\n        # add discounted Q value of next state to non-terminal states (i.e. not crashed)\n        target[~crashes] += self.discount * self.target_dqn.get_action_and_q(states_next[~crashes])[1]\n        self.main_dqn.train(states, actions, target)\n\nNetwork architecture\nThe Deep Convolutional NN architecture used to solve the T-rex game is based on [2] but is extended with a dueling structure. It contains three convolution layers with ReLu activation functions, one max pooling layer and two fully connected layers. The state (i.e. four stacked images) first go through a convolution layer with 32 filters of size 8×8 with stride 4, followed by a ReLU layer. Then a 2×2 max pooling is applied to the output of convolution. The tensor then go through two convolution layers with 64 filters of size 4×4, stride 2 and 64 filters of size 3×3, stride 1. We then flatten the tensor to pass it through a dueling layer.\nThe idea behind a dueling structure is to decompose the Q value into two parts. The first is the value function  which indicades the value of being in a certain state. The second is the advantage function  which tells how much better taking a certain action would be compared to the others. We can then think of . The goal of Dueling DQN is to have a network that separately computes the advantage and value functions, and combines them back into a single Q-function only at the final layer [7, 8]. We achieve this by implementing two fully connected layers for both the value and advantage function. We then average the output of these two parts to end up with the final Q value.\nThe network logic is implemented in the DQN class.\n\n\nReward function\nThe reward function I used to train the model looks as follows:\nif crashed:\n    reward = -100\nelse:\n    if action == UP:\n        reward = -5\n    elif action == DOWN:\n        reward = -3\n    else:\n        reward = 1\nIt favors going forward over jumping and ducking and ducking over jumping. This reward helps the model understand that random actions when there are no obstacles are unnecessary. In earlier attempts I used another pretty straightforward reward function: -100 if the dino crashes else +1. While I think that this reward function should work in practice, it leads to a lot of spurious jumps and ducks."
  },
  {
    "objectID": "notes/tf-rex.html#driver",
    "href": "notes/tf-rex.html#driver",
    "title": "TF-rex: Playing Google’s T-rex game with TensorFlow",
    "section": "Driver",
    "text": "Driver\nFinally, in the main.py file you will find the code that initialises the different components, such as the agent, the environment and the preprocessor, and starts the learning loop. There is also some code to checkpoint the models (i.e. save the current state of the neural network in order to restore it later) and to send some interesting statistics to Tensorboards in order to monitor the experiments.\nepoch = 0\nwhile True:\n    epoch += 1\n\n    frame, _ , crashed = env.start_game()\n    frame = preprocessor.process(frame)\n    state = preprocessor.get_initial_state(frame)\n    ep_steps, ep_reward = 0, 0\n\n    while not crashed:\n\n        action, explored = agent.act(state)\n        next_frame, reward, crashed = env.do_action(action)\n        next_frame = preprocessor.process(next_frame)\n        next_state = preprocessor.get_updated_state(next_frame)\n        agent.remember(state, action, reward, next_state, crashed)\n\n        ep_steps += 1\n        ep_reward += reward\n        state = next_state\n\n    agent.replay(epoch)\n    agent.explore_less()\n    agent.update_target_network()"
  },
  {
    "objectID": "notes/tf-rex.html#results",
    "href": "notes/tf-rex.html#results",
    "title": "TF-rex: Playing Google’s T-rex game with TensorFlow",
    "section": "Results",
    "text": "Results\nTraining a TF-rex model takes about half a day. As we are interacting with the real game, we can not speed-up this process – the TF-rex just has to play the games one-by-one. After training, TF-rex typically reaches a score of around 1600, which is reasonably good, but on average worse than when I play the game. The reason for this is that the speed of the game changes over time. At the start of the game the obstacles approach the dino at a relatively slow speed, but as the game advances the obstacles move faster and faster towards the dino. This causes two problems:\n\nThe probability that the dino dies before entering the increased velocity part is high. This limits the number of high-speed SARSA elements in the experience replay buffer, making our Memory highly unbalanced. As a result, when we sample uniformly from this memory we mainly end up with low-speed transitions that are used to optize the Bellman equation error. Effectively overlooking the behaviour of TF-rex in high-speed parts.\nExploration in these high-speed parts is typically catastrophical. Any uncontrolled jump or duck will result in a crash. This limits the dino to learn new behaviour in the changing environment.\n\nThese problems should be addressed, I’m open for suggestions. I believe Prioritized Experience Replay [9] could proof useful.\n\nProgress whilst learning\n\n\n\n\nRuns of trained model"
  },
  {
    "objectID": "notes/tf-rex.html#references",
    "href": "notes/tf-rex.html#references",
    "title": "TF-rex: Playing Google’s T-rex game with TensorFlow",
    "section": "References",
    "text": "References\n[1] Playing Atari with Deep Reinforcement Learning  [2] AI for Chrome Offline Game  [3] IAMDinosaur  [4] Wikipedia page RL  [5] Open AI Gym  [6] RL course David Silver  [7] RL blogpost  [8] Target network paper  [9] Prioritized Experience Replay\n\nThanks!\nThanks for reading this lengthy post about my project. Hope you enjoyed it and found it useful in some way. If you have any question, please don’t hesitate to drop me an email or find me on twitter."
  },
  {
    "objectID": "talks/content/sumo/presentation.html#hi-im-vincent",
    "href": "talks/content/sumo/presentation.html#hi-im-vincent",
    "title": "",
    "section": "Hi, I’m Vincent 👋",
    "text": "Hi, I’m Vincent 👋"
  },
  {
    "objectID": "talks/content/sumo/presentation.html#hi-im-vincent-1",
    "href": "talks/content/sumo/presentation.html#hi-im-vincent-1",
    "title": "",
    "section": "Hi, I’m Vincent 👋",
    "text": "Hi, I’m Vincent 👋\n\n\nFinished my Bachelor’s and Masters at Ghent University in 2017\nMoved to Cambridge (UK) straight after to work for a startup PROWLER.io\nStarted my PhD at Cambridge University in 2020\nSenior ML researcher at Secondmind.ai"
  },
  {
    "objectID": "talks/content/sumo/presentation.html#outline",
    "href": "talks/content/sumo/presentation.html#outline",
    "title": "",
    "section": "Outline",
    "text": "Outline\nThe talk will consist of two parts:\n\n\nA tutorially overview of energy-based models, and how this has led to diffusion models.\nDiffusion models for stochastic processes.\n\n\n\n\nCredits to Yang Song, Michael Hutchinson, and Arnaud Doucet for some of the images and slide material."
  },
  {
    "objectID": "talks/content/sumo/presentation.html#generative-modelling",
    "href": "talks/content/sumo/presentation.html#generative-modelling",
    "title": "",
    "section": "Generative modelling",
    "text": "Generative modelling\n\n\nGiven: dataset \\(\\{\\v{x}_i\\}_{i=1}^n\\)\nGoal: fit a model \\(p_\\theta(\\v{x})\\) to the data distribution\n\n\n\n\n\n\nIllustration generative modelling (from: openai.com)"
  },
  {
    "objectID": "talks/content/sumo/presentation.html#energy-based-models",
    "href": "talks/content/sumo/presentation.html#energy-based-models",
    "title": "",
    "section": "Energy-based models",
    "text": "Energy-based models\nA density defined through an energy function \\(U_\\theta: \\R^d \\rightarrow \\R\\): \\[\np_{\\theta}(\\v{x}) = \\frac{\\textrm{e}^{-U_\\theta(x)}}{Z_\\theta},\\quad\n\\]\n\nWe can fit this energy function by maximising the log likelihood: \\[\\begin{equation}\n    \\theta^* = \\max_\\theta \\sum_{i=1}^N \\log p_\\theta(\\v{x}_i)\n\\end{equation}\\]\n\n\n️⚠️️ Intractable normalizing constant: \\(Z_\\theta = \\int_{\\R^d} \\textrm{e}^{-U_\\theta(\\v{x})}\\d \\v{x}\\)."
  },
  {
    "objectID": "talks/content/sumo/presentation.html#score-function",
    "href": "talks/content/sumo/presentation.html#score-function",
    "title": "",
    "section": "Score function",
    "text": "Score function\nModelling the density through the score function\n\\[\n\\nabla_{\\v{x}} \\log p(\\v{x})\n\\approx s_\\theta(\\v{x}).\n\\]\n\nThe score does not depend on the normalizing constant. Let \\(p(x) = \\frac{q(x)}{Z}\\)\n\n\n\\[\n\\nabla_x \\log \\frac{q(x)}{Z}  = \\nabla_x \\log q(x) - \\colorbox{white}{$\\nabla_x \\log Z$}\n\\]\n\n\n\\[\n\\nabla_x \\log \\frac{q(x)}{Z}  = \\nabla_x \\log q(x) - \\underbrace{\\colorbox{orange}{$\\nabla_x \\log Z$}}_{= 0}\n\\]\n\n\n\n\n\n\n\n\n\n\n\\(p(x)\\)\n\n\n\n\n\n\n\n\n\\(\\nabla_x \\log p(x)\\)"
  },
  {
    "objectID": "talks/content/sumo/presentation.html#langevin-dynamics",
    "href": "talks/content/sumo/presentation.html#langevin-dynamics",
    "title": "",
    "section": "Langevin dynamics",
    "text": "Langevin dynamics\n\nTheorem 1 The density of \\(\\v{x}_t\\) as \\(t\\rightarrow\\infty\\) for the SDE \\[\n\\d \\v{x}_t = \\nabla \\log p(\\v{x}_t) \\d t + \\sqrt{2} \\d \\v{W}_t\n\\] is given by \\(p(\\v{x})\\), where \\(\\v{W}_t\\) is standard Brownian motion.\n\n\nEuler-Marayuma\nFirst-order discretization of continuous SDE. For a small stepsize \\(\\gamma\\) we can \\[\n\\v{x}_{k+1} = \\v{x}_k + \\gamma \\nabla \\log p(\\v{x}_k) + \\sqrt{2}\\v{z}_k,\\quad \\v{z}_k \\sim \\c{N}(0,\\gamma \\m{I})\n\\]"
  },
  {
    "objectID": "talks/content/sumo/presentation.html#fisher-divergence",
    "href": "talks/content/sumo/presentation.html#fisher-divergence",
    "title": "",
    "section": "Fisher divergence",
    "text": "Fisher divergence\nWe can train score-based models by minimizing the Fisher divergence between the model and the data distributions\n\\[\n    \\E_{p(\\v{x})}\\Big[{\\|\\nabla_{\\v{x}} \\log p(\\v{x})\\ - {\\mathbf{s}_\\theta}(\\v{x})\\|^2}\\Big].\n\\]\n\n\n\nInfeasible because it requires access to the unknown data score.\nScore-matching: Hyvärinen (2005), Vincent (2011), Song et al. (2019)"
  },
  {
    "objectID": "talks/content/sumo/presentation.html#learning-the-score-explicit",
    "href": "talks/content/sumo/presentation.html#learning-the-score-explicit",
    "title": "",
    "section": "Learning the score (Explicit)",
    "text": "Learning the score (Explicit)\nWe would like to explicitly match a parametric score to the (true) score, minimising, with a loss like \\[\\begin{equation}\n    \\ell_{\\mathrm{esm}}({\\mathbf{s}_\\theta}) \\triangleq \\E_{p(\\v{x})}\\Big[{\\|\\nabla_{\\v{x}} \\log p(\\v{x})\\ - {\\mathbf{s}_\\theta}(\\v{x})\\|^2}\\Big].\n\\end{equation}\\]\n\n⚠️ Requires having the true score of \\(p(\\v{x})\\)…"
  },
  {
    "objectID": "talks/content/sumo/presentation.html#learning-the-score-implicit",
    "href": "talks/content/sumo/presentation.html#learning-the-score-implicit",
    "title": "",
    "section": "Learning the score (Implicit)",
    "text": "Learning the score (Implicit)\nThe explicit objective can be written without requiring the true score\n(Hyvärinen 2005)\n\\[\\begin{equation}\n    \\ell_{\\mathrm{ism}}({\\mathbf{s}_\\theta}) \\triangleq \\E_{p(\\v{x})}\\Big[\\nabla_{\\v{x}} \\cdot {\\mathbf{s}_\\theta}(\\v{x}) + \\frac{1}{2}\\|{\\mathbf{s}_\\theta}(\\v{x})\\|^2\\Big] = \\frac{1}{2} \\ell_{\\mathrm{esm}}({\\mathbf{s}_\\theta})  + C\n\\end{equation}\\]"
  },
  {
    "objectID": "talks/content/sumo/presentation.html#naive-score-based-generative-modeling",
    "href": "talks/content/sumo/presentation.html#naive-score-based-generative-modeling",
    "title": "",
    "section": "Naive score-based generative modeling",
    "text": "Naive score-based generative modeling\n\\[\n\\]\n\nScore-based generative modeling with score matching + Langevin dynamics. (from: Yang Song)"
  },
  {
    "objectID": "talks/content/sumo/presentation.html#pitfalls",
    "href": "talks/content/sumo/presentation.html#pitfalls",
    "title": "",
    "section": "Pitfalls",
    "text": "Pitfalls\n\\[\n\\]\n\n\n\nScore is badly estimated in low-density areas\n\n\n\n\nLangevin dynamics has slow mixing rates"
  },
  {
    "objectID": "talks/content/sumo/presentation.html#multiple-noise-levels",
    "href": "talks/content/sumo/presentation.html#multiple-noise-levels",
    "title": "",
    "section": "Multiple noise levels",
    "text": "Multiple noise levels\n\nGaussian noise to perturb the data distributionSong and Ermon (2019) suggest to perturb data points such that they populate low data density regimes."
  },
  {
    "objectID": "talks/content/sumo/presentation.html#markov-chain",
    "href": "talks/content/sumo/presentation.html#markov-chain",
    "title": "",
    "section": "Markov chain",
    "text": "Markov chain\n\n\nConsider a Markov chain \\(\\v{x}_0 \\sim p_{0}\\) and \\(\\v{x}_{k+1} \\sim p_{k+1|k}(\\cdot | \\v{x}_k)\\), which gives\n\n\n\nForward\n\\[\np(\\v{x}_{0:K}) = p_0(\\v{x}_0) \\prod_{k=0}^{K-1} p_{k+1|k}(\\v{x}_{k+1} | \\v{x}_k)\n\\]\n\n\n\nBackward\n\\[\np(\\v{x}_{0:K}) = p_K(\\v{x}_K) \\prod_{k=K-1}^{0} p_{k|k+1}(\\v{x}_{k} | \\v{x}_{k+1})\n\\]\n\nwhere \\(p_{k|k+1}(\\v{x}_{k} | \\v{x}_{k+1})\\) is unknown but can be obtained with Bayes’ rule."
  },
  {
    "objectID": "talks/content/sumo/presentation.html#generative-modelling-with-multiple-noise-levels",
    "href": "talks/content/sumo/presentation.html#generative-modelling-with-multiple-noise-levels",
    "title": "",
    "section": "Generative modelling with multiple noise levels",
    "text": "Generative modelling with multiple noise levels\n\nLet \\[\np_0 = p_{data}\n\\]\nChoose \\[\np_{k+1|k}(\\v{x}_{k+1} | \\v{x}_k) = \\c{N}(\\v{x}_{k+1}| \\alpha \\v{x}_k, (1 - \\alpha^2)\\m{I})\n\\]\nsuch that for large enough \\(K\\) we have \\[\np_K \\approx p_{ref} = \\c{N}(0, \\m{I}).\n\\]"
  },
  {
    "objectID": "talks/content/sumo/presentation.html#backward-transition",
    "href": "talks/content/sumo/presentation.html#backward-transition",
    "title": "",
    "section": "Backward transition",
    "text": "Backward transition\n\nFor sampling we need the reverse kernel \\(p_{k|k+1}\\), given through Bayes’ rule \\[\np_{k|k+1}(\\v{x}_{k} | \\v{x}_{k+1}) =  \\frac{p_{k+1|k}(\\v{x}_{k+1}|\\v{x}_k)p_k(\\v{x}_k)}{p_{k+1}(\\v{x}_{k+1})}\n\\] which is unfortunately intractable!\n\n\n\nUsing a Taylor approximation one can show \\[\np_{k|k+1}(\\v{x}_{k} | \\v{x}_{k+1}) \\approx \\c{N}\\Big(\\v{x}_k | (2 - \\alpha) \\v{x}_{k+1} + (1-\\alpha^2)\n\\colorbox{orange}{$\\nabla \\log p_{k+1}(\\v{x}_{k+1})$},\n(1-\\alpha^2)\\m{I}\\Big)\n\\]\n\n\n\n\nApproximate score with neural net \\(s_{\\theta}(\\v{x}_{k+1}, k+1) \\approx \\nabla \\log p_{k+1}(\\v{x}_{k+1})\\).\n\n\n\n\nSampling start with \\(\\v{x}_K \\sim p_{ref}(\\v{x}_K)\\) and then uses the reverse kernel \\[\n\\v{x}_k = (2 - \\alpha) \\v{x}_{k+1} + (1-\\alpha^2) s_{\\theta}(\\v{x}_{k+1}, k+1) + (1-\\alpha^2)\\v{\\epsilon}\n\\qquad \\v{\\epsilon} \\sim \\c{N}(\\v{0},\\m{I}).\n\\]"
  },
  {
    "objectID": "talks/content/sumo/presentation.html#score",
    "href": "talks/content/sumo/presentation.html#score",
    "title": "",
    "section": "Score",
    "text": "Score\nThe score \\(\\nabla \\log p_k(\\v{x}_k)\\) is required but analytically unavailable. However, using\n\\[\np_{k}(\\v{x}_{k}) = \\int p(\\v{x}_0) p_{k|0}(\\v{x}_{k} | \\v{x}_0) \\d \\v{x}_{0}\n\\]\nit follows\n\\[\n\\nabla \\log p_{k}(\\v{x}_{k}) = \\mathbb{E}_{\\v{x}_0 \\sim  p(\\cdot | \\v{x}_k)}\\Big[ \\nabla \\log p_{k|0}(\\v{x}_k | \\v{x}_0) | \\v{x}_k \\Big]\n\\]\n\nA conditional expectation can be written as a regression problem (by definition), which gives \\[\n\\nabla \\log p_{k}(\\v{x}_{k}) = \\textrm{argmin}_{\\theta}\\,\\mathbb{E}_{\\v{x}_0,\\v{x}_{k}} \\Big[ \\| s_{\\theta}(\\v{x}_{k}) - \\nabla_{\\v{x}_k}\\log p_{k|0}(\\v{x}_{k}|\\v{x}_0)\\|^2\\Big]\n\\]"
  },
  {
    "objectID": "talks/content/sumo/presentation.html#annealed-langevin-dynamics",
    "href": "talks/content/sumo/presentation.html#annealed-langevin-dynamics",
    "title": "",
    "section": "Annealed Langevin Dynamics",
    "text": "Annealed Langevin Dynamics\n\n\\(\\approx\\) Noise Conditional Score Network (NCSN) by Song and Ermon (2019)\n\\(\\approx\\) Denoising Diffusion Probabilistic Models (DDPM) by Ho, Jain, and Abbeel (2020)\n\n\n\n\n\n\n\nCeleb A\n\n\n\n\n\n\n\n\nCIFAR-10"
  },
  {
    "objectID": "talks/content/sumo/presentation.html#perturbing-data-with-an-sde-in-continuous-time",
    "href": "talks/content/sumo/presentation.html#perturbing-data-with-an-sde-in-continuous-time",
    "title": "",
    "section": "Perturbing data with an SDE in continuous time",
    "text": "Perturbing data with an SDE in continuous time\nFrom a (large) discrete set of noise scales \\(\\rightarrow\\) continuous number.\n\n\n\n\nForward SDE runs\n\n\n\n\nThe SDE can be written as \\[\n\\d \\v{x}_t = f(\\v{x}_t, t) \\d t + g(t) \\d \\m{W}_t,\\qquad \\v{x}_0 \\sim p_{data}\n\\] where \\(f\\) and \\(g\\) are the drift and diffusion terms, and \\(\\m{W}_t\\) is standard Brownian motion. Heuristically, you can think of it as “\\(\\d \\m{W}/\\d t \\sim \\c{N}(0, \\d t)\\)”."
  },
  {
    "objectID": "talks/content/sumo/presentation.html#reversing-the-sde-for-sample-generation",
    "href": "talks/content/sumo/presentation.html#reversing-the-sde-for-sample-generation",
    "title": "",
    "section": "Reversing the SDE for sample generation",
    "text": "Reversing the SDE for sample generation\n\n\n\n\nGenerating data following the reverse SDE\n\n\n\nReverse process (Nelson’s duality)\n\\[\n\\d \\v{x}_t = \\big[f(\\v{x}_t, t) - g^2(t) \\nabla_\\v{x} \\log p_t(\\v{x})\\big] \\d t + g(t) \\d \\bar{\\m{W}}_t,\\quad \\v{x}_T \\sim p_T\n\\] where \\(\\d t\\) represents a negative infinitesimal time step as \\(t=T \\rightarrow 0\\)."
  },
  {
    "objectID": "talks/content/sumo/presentation.html#generative-modelling-by-approximating-the-reverse-process",
    "href": "talks/content/sumo/presentation.html#generative-modelling-by-approximating-the-reverse-process",
    "title": "",
    "section": "Generative modelling by approximating the reverse process",
    "text": "Generative modelling by approximating the reverse process\nExact reverse process\n\\[\n\\d \\v{x}_t = \\big[f(\\v{x}_t, t) - g^2(t) \\colorbox{orange}{$\\nabla_\\v{x} \\log p_t(\\v{x})$}\\big] \\d t + g(t) \\d \\bar{\\m{W}}_t,\\quad \\v{x}_T \\sim \\colorbox{lightgray}{$p_T$}\n\\]\nGenerative model\n\\[\n\\d \\v{x}_t = \\big[f(\\v{x}_t, t) - g^2(t) \\colorbox{orange}{$s_{\\theta^*}(\\v{x}_t, t)$}\\big] \\d t + g(t) \\d \\bar{\\m{W}}_t,\\quad \\v{x}_T \\sim \\colorbox{lightgray}{$p_{ref}$}\n\\]\n\nThe score is learned using score-matching, similar to before\n\\[\n\\theta^* = \\textrm{argmin}_{\\theta} \\Big[\\mathbb{E}_{\\v{x}_0,\\v{x}_{t}} \\| s_{\\theta}(\\v{x}_{t}, t) - \\nabla_{\\v{x}_t}\\log p_{t|0}(\\v{x}_{t}|\\v{x}_0)\\|^2\\Big]\n\\]\n\n\nFor OU processes \\(p_{t|0}(\\v{x}_{t}|\\v{x}_0)\\) can analytically be computed using the Fokker-Planck equations and leads to simple expression of the form \\[\np_{t|0}(\\v{x}_{t}|\\v{x}_0) = \\c{N}\\big(\\v{x}_t; \\textrm{e}^{-t} \\v{x}_0, (1-\\textrm{e}^{-2t}) \\m{I}\\big)\n\\]"
  },
  {
    "objectID": "talks/content/sumo/presentation.html#continuous-time-denoising-song2021scorebased",
    "href": "talks/content/sumo/presentation.html#continuous-time-denoising-song2021scorebased",
    "title": "",
    "section": "Continuous-time denoising — Song et al. (2021)",
    "text": "Continuous-time denoising — Song et al. (2021)\n\nForward-Reverse\n\nContinuous-time formulation generalizes the discrete approaches.\n\n\n\n\nLog-likelihood computations \\(\\log p_\\theta(\\v{x}_0)\\) (not shown here)."
  },
  {
    "objectID": "talks/content/sumo/presentation.html#motivation",
    "href": "talks/content/sumo/presentation.html#motivation",
    "title": "",
    "section": "Motivation",
    "text": "Motivation\n\nDiffusion models have been used on different data modalities:"
  },
  {
    "objectID": "talks/content/sumo/presentation.html#diffusion-models-for-functions",
    "href": "talks/content/sumo/presentation.html#diffusion-models-for-functions",
    "title": "",
    "section": "Diffusion models for ‘functions’",
    "text": "Diffusion models for ‘functions’\n\nDistribution over functions"
  },
  {
    "objectID": "talks/content/sumo/presentation.html#perturbing-function-values-using-ou-process",
    "href": "talks/content/sumo/presentation.html#perturbing-function-values-using-ou-process",
    "title": "",
    "section": "Perturbing function-values using OU process",
    "text": "Perturbing function-values using OU process\nLet \\(\\m{X} \\in \\R^{n \\times d}\\) and \\(\\v{y}(\\v{X}) \\in \\R^n\\), we define the forward noising process as \\[\n\\d \\v{y}_t(\\m{X}) = -\\frac{1}{2} \\v{y}_t(\\m{X}) \\d t + \\d \\m{W}_t\n\\]\nOur random variable \\(\\{\\v{y}_t\\}_{t=0}^T\\) is now a function which depends on inputs \\(\\m{X}\\).\n\nUsing Fokker-Planck, we can compute the marginal density in closed-form for for this process \\[\np_{t|0}(\\v{y}_t(\\m{X}) | \\v{y}_0(\\m{X})) = \\c{N}\\Big(\\textrm{e}^{-\\frac{1}{2} \\v{y}_0(\\m{X})}, (1 - \\textrm{e}^{-t}) \\m{I} \\Big)\n\\]"
  },
  {
    "objectID": "talks/content/sumo/presentation.html#forward-process",
    "href": "talks/content/sumo/presentation.html#forward-process",
    "title": "",
    "section": "Forward process",
    "text": "Forward process\n\nForward NDP process"
  },
  {
    "objectID": "talks/content/sumo/presentation.html#reverse-process",
    "href": "talks/content/sumo/presentation.html#reverse-process",
    "title": "",
    "section": "Reverse process",
    "text": "Reverse process\n\n\nExact reverse process \\[\n\\d \\v{x}_t = \\big[f(\\v{x}_t, t) - g^2(t) \\colorbox{orange}{$\\nabla_\\v{x} \\log p_t(\\v{x})$}\\big] \\d t + g(t) \\d \\bar{\\m{W}}_t,\\quad \\v{x}_T \\sim \\colorbox{lightgray}{$p_T$}\n\\]\n\n\nGenerative model \\[\n\\d \\v{x}_t = \\big[f(\\v{x}_t, t) - g^2(t) \\colorbox{orange}{$s_{\\theta^*}(\\v{x}_t, t)$}\\big] \\d t + g(t) \\d \\bar{\\m{W}}_t,\\quad \\v{x}_T \\sim \\colorbox{lightgray}{$p_{ref}$}\n\\]"
  },
  {
    "objectID": "talks/content/sumo/presentation.html#learning-the-score",
    "href": "talks/content/sumo/presentation.html#learning-the-score",
    "title": "",
    "section": "Learning the score",
    "text": "Learning the score\nThe minima of the Fisher divergence can be shown to be equivalent to \\[\n\\theta^* = \\textrm{argmin}_{\\theta}\\,\\mathbb{E}_{\\v{y}_0,\\v{y}_{t}} \\Big[\n     \\| s_{\\theta}(\\v{y}_{t}, \\m{X}, t) - \\nabla_{\\v{y}_t}\\log p_{t|0}(\\v{y}_{t}|\\v{y}_0)\\|^2\n\\Big]\n\\]\n\n\n\n\nScore network architecture\n\n\nEncode properties of stochastic processes in score network \\(s_\\theta\\):\n\ndimensionality invariance\nexchangeability"
  },
  {
    "objectID": "talks/content/sumo/presentation.html#predictions",
    "href": "talks/content/sumo/presentation.html#predictions",
    "title": "",
    "section": "Predictions",
    "text": "Predictions\nWe use an algorithm similar to the one used for image inpainting by Lugmayr et al. (2022).\nThis allows us to condition samples on observed data:\n\n\n\n\nNeural Diffusion Process\n\n\n\n\n\n\n\nGaussian Processes"
  },
  {
    "objectID": "talks/content/sumo/presentation.html#experiment-capturing-non-gaussian-posteriors",
    "href": "talks/content/sumo/presentation.html#experiment-capturing-non-gaussian-posteriors",
    "title": "",
    "section": "Experiment: Capturing non-Gaussian posteriors",
    "text": "Experiment: Capturing non-Gaussian posteriors\nConsider the following distribution over functions. Let \\(a \\sim \\c{U}[-1, 1]\\) then \\[\nf(x) = 0.0 \\text{ if } x < a\\text{, else } 1.0\n\\]"
  },
  {
    "objectID": "talks/content/sumo/presentation.html#experiment-image-regression",
    "href": "talks/content/sumo/presentation.html#experiment-image-regression",
    "title": "",
    "section": "Experiment: Image Regression",
    "text": "Experiment: Image Regression\nLearning complex covariances from data."
  },
  {
    "objectID": "talks/content/sumo/presentation.html#references",
    "href": "talks/content/sumo/presentation.html#references",
    "title": "",
    "section": "References",
    "text": "References\n\n\nDutordoir, Vincent, Alan Saul, Ghahramani Zoubin, and Fergus Simpson. 2022. “Neural Diffusion Processes.” arXiv.\n\n\nHo, Jonathan, Ajay Jain, and Pieter Abbeel. 2020. “Denoising Diffusion Probabilistic Models.” Advances in Neural Information Processing Systems 33: 6840–51.\n\n\nHyvärinen, Aapo. 2005. “Estimation of Non-Normalized Statistical Models by Score Matching.” Journal of Machine Learning Research 6 (24): 695–709. http://jmlr.org/papers/v6/hyvarinen05a.html.\n\n\nLugmayr, Andreas, Martin Danelljan, Andres Romero, Fisher Yu, Radu Timofte, and Luc Van Gool. 2022. “RePaint: Inpainting Using Denoising Diffusion Probabilistic Models.” arXiv. https://doi.org/10.48550/arXiv.2201.09865.\n\n\nSong, Yang, and Stefano Ermon. 2019. “Generative Modeling by Estimating Gradients of the Data Distribution.” Advances in Neural Information Processing Systems 32.\n\n\nSong, Yang, Sahaj Garg, Jiaxin Shi, and Stefano Ermon. 2019. “Sliced Score Matching: A Scalable Approach to Density and Score Estimation,” May.\n\n\nSong, Yang, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. 2021. “Score-Based Generative Modeling Through Stochastic Differential Equations.” In International Conference on Learning Representations.\n\n\nVincent, Pascal. 2011. “A Connection Between Score Matching and Denoising Autoencoders.” Neural Computation 23 (7): 1661–74."
  }
]