<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.335">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2018-09-01">
<meta name="description" content="Deep Q-Learning from scratch in TensorFlow to play Google T-Rex Game">

<title>TF-rex: Playing Google’s T-rex game with TensorFlow</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>


<link rel="stylesheet" href="../styles.css">
<meta name="twitter:title" content="TF-rex: Playing Google’s T-rex game with TensorFlow">
<meta name="twitter:description" content="Deep Q-Learning from scratch in TensorFlow to play Google T-Rex Game">
<meta name="twitter:creator" content="@vdutor">
<meta name="twitter:card" content="summary">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../index.html">
 <span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../talks">
 <span class="menu-text">Talks</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../notes">
 <span class="menu-text">Notes</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#overview" id="toc-overview" class="nav-link active" data-scroll-target="#overview">Overview</a></li>
  <li><a href="#implementation" id="toc-implementation" class="nav-link" data-scroll-target="#implementation">Implementation</a>
  <ul class="collapse">
  <li><a href="#creating-the-rl-environment" id="toc-creating-the-rl-environment" class="nav-link" data-scroll-target="#creating-the-rl-environment">Creating the RL environment</a></li>
  <li><a href="#architecture" id="toc-architecture" class="nav-link" data-scroll-target="#architecture">Architecture</a></li>
  </ul></li>
  <li><a href="#reinforcement-learning-rl" id="toc-reinforcement-learning-rl" class="nav-link" data-scroll-target="#reinforcement-learning-rl">Reinforcement Learning (RL)</a>
  <ul class="collapse">
  <li><a href="#preprocessing" id="toc-preprocessing" class="nav-link" data-scroll-target="#preprocessing">Preprocessing</a></li>
  <li><a href="#learning" id="toc-learning" class="nav-link" data-scroll-target="#learning">Learning</a></li>
  </ul></li>
  <li><a href="#driver" id="toc-driver" class="nav-link" data-scroll-target="#driver">Driver</a></li>
  <li><a href="#results" id="toc-results" class="nav-link" data-scroll-target="#results">Results</a></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references">References</a>
  <ul class="collapse">
  <li><a href="#thanks" id="toc-thanks" class="nav-link" data-scroll-target="#thanks">Thanks!</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">TF-rex: Playing Google’s T-rex game with TensorFlow</h1>
</div>

<div>
  <div class="description">
    Deep Q-Learning from scratch in TensorFlow to play Google T-Rex Game
  </div>
</div>


<div class="quarto-title-meta">

    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">September 1, 2018</p>
    </div>
  </div>
  
    
  </div>
  

</header>

<p>The goal of this project is to play Google’s offline T-rex Dino game using Deep Q-Learning. We use TensorFlow (TF) – hence the name TF-rex ;)</p>
<p>All source code and trained models can be found on <a href="https://github.com/vdutor/TF-rex">GitHub</a>.</p>
<p>The goal of this project is to play Google’s offline T-rex Dino game using Reinforcement Learning (RL). The RL algorithm is based on the Deep Q-Learning algorithm [1] and is implemented in TensorFlow (TF), hence the name TF-rex ;).</p>
<p>Google’s offline game consists of a T-rex striving to dodge obstacles, such as cactuses and birds, and surviving as long as possible. The dino is able to perform three actions: “jumping”, “ducking” and “going forward”. <a href="http://www.trex-game.skipser.com/">You can try it yourself</a>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://github.com/wayou/t-rex-runner/blob/gh-pages/assets/screenshot.gif?raw=true" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">T-Rex gane played by Q-learner</figcaption><p></p>
</figure>
</div>
<p>In this post, I’ll talk about the project’s implementation, the RL setup and algorithms and the training procedure. We assume that the reader is familiar with the basics of RL. If not, no worries! <a href="https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0">This blogpost from Arthur Juliani</a> will quickly get you up to speed with some of the essentials.</p>
<section id="overview" class="level2">
<h2 class="anchored" data-anchor-id="overview">Overview</h2>
<p>We want to create an “AI algorithm” that can play Google Chrome’s T-rex game. This project isn’t the first one to try this, but compared to existing projects [2, 3], we do two things differently. First, the AI is interacting in <em>real-time</em> with the <em>real</em> game, hosted in the browser. We are not using any sort of emulator or framework to slow down the game or frame rate. While this makes the framework genuinely more interesting and more directly applicable to other browser-based games, it requires some extra engineering. For example, we need to extract the game state from the browser and implement a duplex channel to allow for communication between the AI program and the browser. This channel is necessary to pass information such as actions and game states.</p>
<p>Secondly, the AI algorithm is trained with images of the game state. This is in contrast to earlier attempts extracting useful features (e.g.&nbsp;distance to next obstacle and length of the next obstacle) to train the AI. In our opinion, this setup resembles a real-life learning environment much closer.</p>
<p>The codebase exists of two larger components: the javascript T-rex game and the AI python program. Both modules are given in the diagram below. They communicate through a bidirectional websocket using predefined commands.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://cdn.rawgit.com/vdutor/TF-rex/da877014/assets/TF-rex-arch-1.svg" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Architecture</figcaption><p></p>
</figure>
</div>
<p>The diagram depicts the typical RL cycle [4]: an agent interacts with its environment in discrete time steps. At each time, the agent receives an observation, which typically includes the reward and state of the game. It then chooses an action from the set of available actions, which it sends to the environment. The environment moves to a new state by executing the action and the reward associated with the transition is determined. The goal of a reinforcement learning agent is to collect as much reward as possible. More on this below.</p>
<p>In the next section “Implementation” we’ll talk about the structure of the code and how we turned the in-browser game into an RL environment. In the “Reinforcement Learning” section the AI algorithm is discussed. In that section we abstract the fact that we are interacting with a real-life game and assume that we have an environment that provides us with the next state and associated reward, given a new action.</p>
</section>
<section id="implementation" class="level2">
<h2 class="anchored" data-anchor-id="implementation">Implementation</h2>
<section id="creating-the-rl-environment" class="level3">
<h3 class="anchored" data-anchor-id="creating-the-rl-environment">Creating the RL environment</h3>
<p>We want to turn the javascript game, which is running inside the browser, into an RL environment. This means that we want to have a similar interface as, for example, OpenAI Gym environments. The Open AI interface looks as follows [5]</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>ob0 <span class="op">=</span> env.reset() <span class="co"># sample environment state, return first observation</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>a0 <span class="op">=</span> agent.act(ob0) <span class="co"># agent chooses first action</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>ob1, rew0, done0, info0 <span class="op">=</span> env.step(a0) <span class="co"># environment returns observation,</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="co"># reward, and boolean flag indicating if the episode is complete.</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>a1 <span class="op">=</span> agent.act(ob1)</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>ob2, rew1, done1, info1 <span class="op">=</span> env.step(a1)</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>...</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>a99 <span class="op">=</span> agent.act(o99)</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>ob100, rew99, done99, info2 <span class="op">=</span> env.step(a99)</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="co"># done99 == True =&gt; terminal</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>To achieve this, we need to conquer some hurdles. We need to… 1. implement a bidirectional communication channel between the browser/javascript code and the Python AI program, 2. perform actions, i.e.&nbsp;let the dino jump and duck. 3. extract the game state from the game and turn it into a parseable format. Let’s tackle these one by one.</p>
<p>To allow for bidirectional communication, we extent the game’s javascript code. We chose to use <code>WebSocket</code> objects as the transportation medium. When the page is done loading (i.e.&nbsp;<code>onDocumentLoad</code> event) the code will try to establish a connection with an entity running on <code>127.0.0.1:9090</code>.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode javascript code-with-copy"><code class="sourceCode javascript"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="kw">function</span> <span class="fu">onDocumentLoad</span>() {</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>    runner <span class="op">=</span> <span class="kw">new</span> <span class="fu">Runner</span>(<span class="st">'.interstitial-wrapper'</span>)<span class="op">;</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>}<span class="op">;</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="bu">document</span><span class="op">.</span><span class="fu">addEventListener</span>(<span class="st">'DOMContentLoaded'</span><span class="op">,</span> onDocumentLoad)<span class="op">;</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="kw">var</span> socket <span class="op">=</span> <span class="kw">new</span> <span class="bu">WebSocket</span>(<span class="st">"ws://127.0.0.1:9090"</span>)<span class="op">;</span> # will connect Python<span class="op">-</span>side server</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The only thing left to do is to create the listening entity on the Python side. In the project’s repo you can find the <code>websocket_server.py</code> file, which contains the implementation of a websocket server. When a <code>Environment</code> object is initialized, it will create such a websocket and order it to listen for connections on <code>127.0.0.1:9090</code>. (Additionally, the socket will be placed in its own thread and communicate with the main thread using a <code>multiprocessing.Queue</code>.) As long the browser doesn’t establish a connection with this socket, the Python program will halt.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Environment:</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Environment is responsible for the communication with the game, through the socket.</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(host, port): <span class="co"># host = 127.0.0.1 and port = 9090</span></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.queue <span class="op">=</span> multiprocessing.Queue()</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.server <span class="op">=</span> WebsocketServer(port, host<span class="op">=</span>host)</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>        thread <span class="op">=</span> threading.Thread(target <span class="op">=</span> <span class="va">self</span>.server.run_forever)</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>        thread.start()</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>  ...</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>As long as both entities -the javascript code inside the browser and the Python WebSocket object- stay alive, we have a duplex communication channel. As a result, on both sides we have functions to send messages and we have callback-methods to process incoming messages. We will use this to solve the next hurdle: executing actions in the game, requested by the AI program.</p>
<p>The javascript snippet below shows the <code>onmessage</code>-callback in the javascript code, which is ran when a new message from the Python code comes in</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode javascript code-with-copy"><code class="sourceCode javascript"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>socket<span class="op">.</span><span class="at">onmessage</span> <span class="op">=</span> <span class="kw">function</span> (<span class="bu">event</span>)</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>{</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">var</span> command <span class="op">=</span> <span class="bu">event</span><span class="op">.</span><span class="at">data</span><span class="op">;</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>    <span class="kw">var</span> runner <span class="op">=</span> <span class="kw">new</span> <span class="fu">Runner</span>()<span class="op">;</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>    <span class="bu">console</span><span class="op">.</span><span class="fu">log</span>(command)<span class="op">;</span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">switch</span> (command) {</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>        <span class="cf">case</span> <span class="st">'STATE'</span><span class="op">:</span></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>            runner<span class="op">.</span><span class="fu">postState</span>(socket)<span class="op">;</span></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>            <span class="cf">break</span><span class="op">;</span></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>        <span class="cf">case</span> <span class="st">'START'</span><span class="op">:</span></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>            <span class="fu">simulateKey</span>(<span class="st">"keydown"</span><span class="op">,</span> <span class="dv">32</span>)<span class="op">;</span> <span class="co">// space</span></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>            <span class="pp">setTimeout</span>(<span class="kw">function</span>() {<span class="fu">simulateKey</span>(<span class="st">"keyup"</span><span class="op">,</span> <span class="dv">32</span>)<span class="op">;</span>}<span class="op">,</span> <span class="dv">1000</span>)<span class="op">;</span></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>            <span class="cf">break</span><span class="op">;</span></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">case</span> <span class="st">'REFRESH'</span><span class="op">:</span></span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>            location<span class="op">.</span><span class="fu">reload</span>(<span class="kw">true</span>)<span class="op">;</span></span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>            <span class="cf">break</span><span class="op">;</span></span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>        <span class="cf">case</span> <span class="st">'UP'</span><span class="op">:</span></span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>            <span class="fu">simulateKey</span>(<span class="st">"keydown"</span><span class="op">,</span> <span class="dv">38</span>)<span class="op">;</span> <span class="co">// arrow up</span></span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>            <span class="pp">setTimeout</span>(<span class="kw">function</span>() {<span class="fu">simulateKey</span>(<span class="st">"keyup"</span><span class="op">,</span> <span class="dv">38</span>)<span class="op">;</span>}<span class="op">,</span> <span class="dv">400</span>)<span class="op">;</span></span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a>            <span class="cf">break</span><span class="op">;</span></span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a>        <span class="cf">case</span> <span class="st">'DOWN'</span><span class="op">:</span></span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a>            <span class="fu">simulateKey</span>(<span class="st">"keydown"</span><span class="op">,</span> <span class="dv">40</span>)<span class="op">;</span> <span class="co">// arrow down</span></span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a>            <span class="pp">setTimeout</span>(<span class="kw">function</span>() {<span class="fu">simulateKey</span>(<span class="st">"keyup"</span><span class="op">,</span> <span class="dv">40</span>)<span class="op">;</span>}<span class="op">,</span> <span class="dv">400</span>)<span class="op">;</span></span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a>            <span class="cf">break</span><span class="op">;</span></span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a>        <span class="cf">default</span><span class="op">:</span></span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a>}<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>You can see that we specified a simple communication-protocol, consisting of 5 commands: ‘STATE’, ‘START’, ‘REFRESH’, ‘UP’ and ‘DOWN’. The latter two, ‘UP’ and ‘DOWN’, are actions used in the game to control the dino. We execute the action by simulating a keypress on the keyboard. This is easily done in javascript using <code>function simulateKey(type, keyCode)</code> in combination with a <code>setTimeout()</code>. For example, when the Python AI program sends the message ‘UP’, the javascript code will first receive the message and subsequently simulate a press on the arrow-up key, which will cause the T-rex to jump.</p>
<p>The first three actions, ‘STATE’, ‘START’ and ‘REFRESH’, are controlling commands. The ‘STATE’ command will issue the javascript code to collect the current game state (i.e.&nbsp;the current image displaying the dino and the obstacles) and send it over the socket to the python side. The <code>postState</code>-function, ran when a ‘STATE’ message is received, looks like this</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode javascript code-with-copy"><code class="sourceCode javascript"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>postState<span class="op">:</span> <span class="kw">function</span> (socket) {</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>    <span class="bu">console</span><span class="op">.</span><span class="fu">log</span>(<span class="st">"in postState function"</span>)<span class="op">;</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">var</span> canvas <span class="op">=</span> <span class="bu">document</span><span class="op">.</span><span class="fu">getElementById</span>(<span class="st">'runner-id'</span>)<span class="op">;</span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>    <span class="kw">var</span> dataUrl <span class="op">=</span> canvas<span class="op">.</span><span class="fu">toDataURL</span>(<span class="st">"image/png"</span>)<span class="op">;</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>    <span class="kw">var</span> state <span class="op">=</span> {</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>        <span class="dt">world</span><span class="op">:</span> dataUrl<span class="op">,</span></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>        <span class="dt">crashed</span><span class="op">:</span> <span class="kw">this</span><span class="op">.</span><span class="at">crashed</span><span class="op">.</span><span class="fu">toString</span>()</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>    socket<span class="op">.</span><span class="fu">send</span>(<span class="bu">JSON</span><span class="op">.</span><span class="fu">stringify</span>(state))</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>    }</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The function will read the canvas - this is the PNG image you see on the screen - and parse it into a base64-encoded string. Next, a state-message is created with the base64-encoded image-string and a boolean indicating whether or not the dino is still alive. The state-message is parsed into JSON format and sent over the socket. On the python side, we parse the state-message as follows</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> json.loads(message)</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>image, crashed <span class="op">=</span> data[<span class="st">'world'</span>], data[<span class="st">'crashed'</span>]</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a><span class="co"># remove data-info at the beginning of the image</span></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>image <span class="op">=</span> re.sub(<span class="st">'data:image/png;base64,'</span>, <span class="st">''</span>, image)</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a><span class="co"># convert image from base64 decoding to 2D numpy array</span></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>image <span class="op">=</span> np.array(Image.<span class="bu">open</span>(BytesIO(base64.b64decode(image)))) <span class="co"># &lt;-- tricky one</span></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a><span class="co"># cast string boolean to python boolean</span></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>crashed <span class="op">=</span> <span class="va">True</span> <span class="cf">if</span> crashed <span class="kw">in</span> [<span class="st">'True'</span>, <span class="st">'true'</span>] <span class="cf">else</span> <span class="va">False</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The most important line in the snippet above is the decoding of the base64 image-string into a 2D image matrix. All the functionality is provided by standard Python libraries, such as PIL (Python Image Library), JSON and base64. However, it took me some time to find the correct ones as weird behaviour occurs easily when passing images between different programming languages or even libraries.</p>
<p>We faced the three obstacles that were required to turn the in-browser game into an RL environment. The <code>Environment</code> class contains most of this logic and provides an easy interface to the game. The most important method is <code>do_action()</code> (similar to <code>act()</code> in Open AI Gyms)</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Environment:</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>  <span class="kw">def</span> <span class="fu">__init__</span>():</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># see above</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>  <span class="kw">def</span> do_action(<span class="va">self</span>, action):</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>     <span class="co">"""</span></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a><span class="co">     Performs an action and returns the next state, reward and crash status</span></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a><span class="co">     """</span></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>     <span class="cf">if</span> action <span class="op">!=</span> Action.FORWARD:</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>         <span class="co"># noting needs to be send when the action is going forward</span></span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>         <span class="va">self</span>.server.send_message(<span class="va">self</span>.game_client, <span class="va">self</span>.actions[action])</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>     time.sleep(<span class="fl">0.1</span>)</span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>     <span class="cf">return</span> <span class="va">self</span>._get_state(action)</span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a> <span class="kw">def</span> _get_state(<span class="va">self</span>, action):</span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a>     <span class="va">self</span>.server.send_message(<span class="va">self</span>.game_client, <span class="st">"STATE"</span>)</span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a>     next_state, crashed <span class="op">=</span> <span class="va">self</span>.queue.get() <span class="co"># &lt;-- halt while queue is empty (waiting for state-message)</span></span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a>     reward <span class="op">=</span> _calculate_reward(action, crashed)</span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a>     <span class="cf">return</span> next_state, reward, crashed</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Besides the <code>do_action()</code> method, a <code>Environment</code>-object can also <code>start()</code> and <code>refresh()</code> the game. In a similar fashion as the Open AI Gym function <code>reset()</code>.</p>
</section>
<section id="architecture" class="level3">
<h3 class="anchored" data-anchor-id="architecture">Architecture</h3>
<p>The diagram below shows the overall architecture of the project. It illustrates the division of the javascript code running inside the browser and the python code, communicating over a websocket. The classes <code>WebSocket</code> and <code>Environment</code> were discussed above. In the next section, we’ll focus on the remaining classes. They are responsible for the actual <em>learning</em>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://cdn.rawgit.com/vdutor/TF-rex/2a30cb32/assets/TF-rex-arch-2.svg" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Architecture</figcaption><p></p>
</figure>
</div>
</section>
</section>
<section id="reinforcement-learning-rl" class="level2">
<h2 class="anchored" data-anchor-id="reinforcement-learning-rl">Reinforcement Learning (RL)</h2>
<p>To teach the dino how to dodge the approaching obstacles we chose a Deep Q-learning approach, proposed in [1] by DeepMind. We briefly discuss this algorithm from a theoretical point-of-view, and explain how we implemented it.</p>
<p>The main idea of Deep Q-learning is to use a (deep) parametric neural network to approximate the Q-function. The Q-function of a Markov decision process (MDP), often denoted by <script type="math/tex"> Q(s,a) </script> gives the expected utility of taking a given action <script type="math/tex"> a </script> in a given state <script type="math/tex"> s </script> and following an optimal policy thereafter. In other words, the Q-function <script type="math/tex"> Q : </script> is a function which takes as arguments an action and a state and returns the expected total future reward of the agent if it would execute the action <script type="math/tex"> a </script> in state <script type="math/tex"> s </script> and continue performing optimal actions. As we wish to maximize our reward in every state we typically execute the action which optimizes the total future reward, namely <script type="math/tex"> (s) = argmax_a Q(s,a) </script>. In the literature, this is referred to as the agent’s policy.</p>
<p>An important property of the Q-function is that <script type="math/tex"> Q(s,a) = r + max_{a’} Q(s’, a’) </script>, where <script type="math/tex"> s’ </script> is the state the agent ends up by performing action <script type="math/tex"> a </script> in state <script type="math/tex"> s </script>, <script type="math/tex"> r </script> is the corresponding reward and <script type="math/tex"> </script> is the discount factor. This equation is referred to as the Bellman equation. We will later use it to train our neural network.</p>
<p>If we let the agent interact with the environment for a while, we end up with a collection of SARSA elements. SARSA stands for State, Action, Reward, State’ and Action’. It holds the current state of the agent S, the action the agent chooses A, the reward R the agent gets for choosing this action, the state S’ that the agent will now be in after taking that action, and finally the next action A’ the agent will choose in its new state. Taking every letter in the quintuple yields the word SARSA. This sequence is depicted in the figure below.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/01/11131906/figtmp7.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">RL loop</figcaption><p></p>
</figure>
</div>
<p>In the context of TF-rex is our action space <script type="math/tex"> </script> limited to three elements: “ducking”, “jumping” and “going forward”. This makes it relatively easy as we only need to deal with a small number of discrete actions. Handling continuous actions or a large space of discrete ones makes the learning typically much harder. The state space, on the other hand, is quite large as it consists out of four stacked images of size 80×80 (i.e.&nbsp;input dimensionality, <script type="math/tex"> D_{in} = 4 = 25,600</script>). In the next section we show how we create these input vectors.</p>
<section id="preprocessing" class="level3">
<h3 class="anchored" data-anchor-id="preprocessing">Preprocessing</h3>
<p>We don’t directly use the images we receive from the javascript game as states. We need to preprocess them before using them as the inputs of the deep Q-learning algorithm. This accelerates the training as we eliminate noisy parts from the image. We also collect multiple images into a single state which serves as a kind of memory.</p>
<p>The image below shows the original version, i.e.&nbsp;the image collected and sent by the javascript game and received by the <code>Environment</code>. It is a grey-scale image and has dimensions 150×600. The highscore and current score are shown in the upper-right corner.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://github.com/vdutor/TF-rex/blob/master/assets/preprocessing-step1.png?raw=true" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Original image</figcaption><p></p>
</figure>
</div>
<p>We apply two preprocessing steps on this image.</p>
<ol type="1">
<li><p>extract a region-of-interest (ROI)</p>
<p>To dodge the obstacles the left part of the image is clearly much more informative than the right side. Therefore, we select <code>roi = image[:, 420]</code>, which drops 30% of the right-side pixels. This operation reduces the number of input pixels, as <code>roi</code> is 150×420 and removes the meta-information in the upper-right corner.</p></li>
<li><p>remove harmless objects</p>
<p>The cloud, which you can see in the middle of the original image, doesn’t hurt the dino. The dino can touch it (i.e.&nbsp;have overlapping pixels) without dying. Harmless obstacles are easily filtered out, as they have a lighter colour than real obstacles, so we chose to remove them from the images using some straightforward masking techniques.</p></li>
</ol>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://github.com/vdutor/TF-rex/blob/master/assets/preprocessing-step2.png?raw=true" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">ROI and Remove clouds</figcaption><p></p>
</figure>
</div>
<p>We then apply a standard preprocessing step (inspired from the Atari games paper): resizing the image to 80×80 grid of pixels.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://github.com/vdutor/TF-rex/blob/master/assets/preprocessing-step3.png?raw=true" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Squaring</figcaption><p></p>
</figure>
</div>
<p>Finally, we stack the last 4 frames in order to produce an 80×80×4 array which serves as a state for the Deep Q-Learning algorithm.</p>
<p>All these operations are performed by the <code>Preprocessor</code> class.</p>
</section>
<section id="learning" class="level3">
<h3 class="anchored" data-anchor-id="learning">Learning</h3>
<p>Like most RL algorithms, Deep Q-learning uses a SARSA observation to get an unbiased estimate of the error <script type="math/tex; mode=display"> () = ( Q_{}(s,a) - (r + ,max_{a’}Q_{}(s’,a’)) )^2, </script> where <script type="math/tex"> Q_{}(s,a) </script> denotes that the Q-function is built out of a neural net with parameters <script type="math/tex"> </script>. Minimizing <script type="math/tex"> () </script> w.r.t. <script type="math/tex"> </script> is used to optimize the neural network parameters.</p>
<p>While this approach works in theory, in practice we see that that during the optimization the neural net tends to oscillate or diverge. See, for example, David Silver’s courses for an explanation why [6]. A couple of tricks are introduced to reduce this unwanted behaviour.</p>
<p><strong>1. Experience Replay</strong></p>
<p>Instead of using only a single SARSA observation in the error function <script type="math/tex"> () </script> we use a batch of observations. This breaks correlations in the data, reduces the variance of the gradient estimator, and allows the network to learn from a more varied array of past experiences.</p>
<p>From an implementation point of view this means that we need to store each observed SARSA element. At training time, we also need to be able to sample from this memory in order to get a new batch of experiences. We implemented a <code>Memory</code> class which does exactly this. We choose to use a FIFO-queue to store the SARSA elements.</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Memory:</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, size):</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.size <span class="op">=</span> size</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.mem <span class="op">=</span> np.ndarray((size,<span class="dv">5</span>), dtype<span class="op">=</span><span class="bu">object</span>)</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.<span class="bu">iter</span> <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.current_size <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> remember(<span class="va">self</span>, state1, action, reward, state2, crashed):</span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.mem[<span class="va">self</span>.<span class="bu">iter</span>, :] <span class="op">=</span> state1, action, reward, state2, crashed</span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.<span class="bu">iter</span> <span class="op">=</span> (<span class="va">self</span>.<span class="bu">iter</span> <span class="op">+</span> <span class="dv">1</span>) <span class="op">%</span> <span class="va">self</span>.size</span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.current_size <span class="op">=</span> <span class="bu">min</span>(<span class="va">self</span>.current_size <span class="op">+</span> <span class="dv">1</span>, <span class="va">self</span>.size)</span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> sample(<span class="va">self</span>, n):</span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a>        n <span class="op">=</span> <span class="bu">min</span>(<span class="va">self</span>.current_size, n)</span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a>        random_idx <span class="op">=</span> random.sample(<span class="bu">list</span>(<span class="bu">range</span>(<span class="va">self</span>.current_size)), n)</span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a>        sample <span class="op">=</span> <span class="va">self</span>.mem[random_idx]</span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> (np.stack(sample[:, i], axis<span class="op">=</span><span class="dv">0</span>) <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">5</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>2. Target Network</strong></p>
<p>In order to avoid oscillations, we will use a different network, called the target network, to estimate the Q-values during several epochs. The target network has fixed parameters, which reduces the variance and makes the learning more stable. We update the target network parameters with the values of our main network periodically. The loss is now calculated as <script type="math/tex; mode=display"> () = ( Q_{}(s,a) - (r + ,max_{a’}Q_{^<em>}(s’,a’)) )^2, </script> where <script type="math/tex"> Q_{}(, ) </script> is our main network and <script type="math/tex"> Q_{^</em>}(, ) </script> is our target network. Importantly, both networks have the same architecture but can have different values for the parameters. Periodically we need to update the target parameters with the newest values, i.e.&nbsp;<script type="math/tex"> ^* </script> . By using this seperate network to compute the targets we get a more stable training procedure as we reduce the number of constantly shifting values in the loss function.</p>
<p>Most of the learning logic is implemented in the <code>DDQNAgent</code> class. Below we show the method that computes the targets. We start by sampling for the experience memory and then calculate <script type="math/tex"> r + ,max_{a’}Q_{^*}(s’,a’)) </script> for every SARSA element in the sample.</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> DDQNAgent:</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> replay(<span class="va">self</span>):</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>        states, actions, rewards, states_next, crashes <span class="op">=</span> <span class="va">self</span>.memory.sample(<span class="va">self</span>.batch_size)</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>        target <span class="op">=</span> rewards</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>        <span class="co"># add discounted Q value of next state to non-terminal states (i.e. not crashed)</span></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>        target[<span class="op">~</span>crashes] <span class="op">+=</span> <span class="va">self</span>.discount <span class="op">*</span> <span class="va">self</span>.target_dqn.get_action_and_q(states_next[<span class="op">~</span>crashes])[<span class="dv">1</span>]</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.main_dqn.train(states, actions, target)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<section id="network-architecture" class="level4">
<h4 class="anchored" data-anchor-id="network-architecture">Network architecture</h4>
<p>The Deep Convolutional NN architecture used to solve the T-rex game is based on [2] but is extended with a <strong>dueling</strong> structure. It contains three convolution layers with ReLu activation functions, one max pooling layer and two fully connected layers. The state (i.e.&nbsp;four stacked images) first go through a convolution layer with 32 filters of size 8×8 with stride 4, followed by a ReLU layer. Then a 2×2 max pooling is applied to the output of convolution. The tensor then go through two convolution layers with 64 filters of size 4×4, stride 2 and 64 filters of size 3×3, stride 1. We then flatten the tensor to pass it through a dueling layer.</p>
<p>The idea behind a dueling structure is to decompose the Q value into two parts. The first is the value function <script type="math/tex"> V(s) </script> which indicades the value of being in a certain state. The second is the advantage function <script type="math/tex"> A(a) </script> which tells how much better taking a certain action would be compared to the others. We can then think of <script type="math/tex"> Q(s,a) = V(s) + A(a) </script>. The goal of Dueling DQN is to have a network that separately computes the advantage and value functions, and combines them back into a single Q-function only at the final layer [7, 8]. We achieve this by implementing two fully connected layers for both the value and advantage function. We then average the output of these two parts to end up with the final Q value.</p>
<p>The network logic is implemented in the <code>DQN</code> class.</p>
</section>
<section id="reward-function" class="level4">
<h4 class="anchored" data-anchor-id="reward-function">Reward function</h4>
<p>The reward function I used to train the model looks as follows:</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> crashed:</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>    reward <span class="op">=</span> <span class="op">-</span><span class="dv">100</span></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a><span class="cf">else</span>:</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> action <span class="op">==</span> UP:</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>        reward <span class="op">=</span> <span class="op">-</span><span class="dv">5</span></span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">elif</span> action <span class="op">==</span> DOWN:</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>        reward <span class="op">=</span> <span class="op">-</span><span class="dv">3</span></span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>        reward <span class="op">=</span> <span class="dv">1</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>It favors going forward over jumping and ducking and ducking over jumping. This reward helps the model understand that random actions when there are no obstacles are unnecessary. In earlier attempts I used another pretty straightforward reward function: -100 if the dino crashes else +1. While I think that this reward function should work in practice, it leads to a lot of spurious jumps and ducks.</p>
</section>
</section>
</section>
<section id="driver" class="level2">
<h2 class="anchored" data-anchor-id="driver">Driver</h2>
<p>Finally, in the <code>main.py</code> file you will find the code that initialises the different components, such as the <code>agent</code>, the <code>environment</code> and the <code>preprocessor</code>, and starts the learning loop. There is also some code to checkpoint the models (i.e.&nbsp;save the current state of the neural network in order to restore it later) and to send some interesting statistics to Tensorboards in order to monitor the experiments.</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>epoch <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="cf">while</span> <span class="va">True</span>:</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>    epoch <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>    frame, _ , crashed <span class="op">=</span> env.start_game()</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>    frame <span class="op">=</span> preprocessor.process(frame)</span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>    state <span class="op">=</span> preprocessor.get_initial_state(frame)</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>    ep_steps, ep_reward <span class="op">=</span> <span class="dv">0</span>, <span class="dv">0</span></span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">while</span> <span class="kw">not</span> crashed:</span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a>        action, explored <span class="op">=</span> agent.act(state)</span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a>        next_frame, reward, crashed <span class="op">=</span> env.do_action(action)</span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a>        next_frame <span class="op">=</span> preprocessor.process(next_frame)</span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a>        next_state <span class="op">=</span> preprocessor.get_updated_state(next_frame)</span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a>        agent.remember(state, action, reward, next_state, crashed)</span>
<span id="cb11-17"><a href="#cb11-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-18"><a href="#cb11-18" aria-hidden="true" tabindex="-1"></a>        ep_steps <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb11-19"><a href="#cb11-19" aria-hidden="true" tabindex="-1"></a>        ep_reward <span class="op">+=</span> reward</span>
<span id="cb11-20"><a href="#cb11-20" aria-hidden="true" tabindex="-1"></a>        state <span class="op">=</span> next_state</span>
<span id="cb11-21"><a href="#cb11-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-22"><a href="#cb11-22" aria-hidden="true" tabindex="-1"></a>    agent.replay(epoch)</span>
<span id="cb11-23"><a href="#cb11-23" aria-hidden="true" tabindex="-1"></a>    agent.explore_less()</span>
<span id="cb11-24"><a href="#cb11-24" aria-hidden="true" tabindex="-1"></a>    agent.update_target_network()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="results" class="level2">
<h2 class="anchored" data-anchor-id="results">Results</h2>
<p>Training a TF-rex model takes about half a day. As we are interacting with the real game, we can not speed-up this process – the TF-rex just has to play the games one-by-one. After training, TF-rex typically reaches a score of around 1600, which is reasonably good, but on average worse than when I play the game. The reason for this is that the speed of the game changes over time. At the start of the game the obstacles approach the dino at a relatively slow speed, but as the game advances the obstacles move faster and faster towards the dino. This causes two problems:</p>
<ol type="1">
<li><p>The probability that the dino dies before entering the increased velocity part is high. This limits the number of high-speed SARSA elements in the experience replay buffer, making our <code>Memory</code> highly unbalanced. As a result, when we sample uniformly from this memory we mainly end up with low-speed transitions that are used to optize the Bellman equation error. Effectively overlooking the behaviour of TF-rex in high-speed parts.</p></li>
<li><p>Exploration in these high-speed parts is typically catastrophical. Any uncontrolled jump or duck will result in a crash. This limits the dino to learn new behaviour in the changing environment.</p></li>
</ol>
<p>These problems should be addressed, I’m open for suggestions. I believe Prioritized Experience Replay [9] could proof useful.</p>
<section id="progress-whilst-learning" class="level4">
<h4 class="anchored" data-anchor-id="progress-whilst-learning">Progress whilst learning</h4>
<iframe width="560" height="315" src="https://www.youtube.com/embed/UM_wyrd0k-k" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen="">
</iframe>
</section>
<section id="runs-of-trained-model" class="level4">
<h4 class="anchored" data-anchor-id="runs-of-trained-model">Runs of trained model</h4>
<iframe width="560" height="315" src="https://www.youtube.com/embed/4B06SSRAf5o" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen="">
</iframe>
<!-- ## TODO

1. Implement more techniques to speed up the learning: e.g. gradient clipping and batch norm.
2. Try the learning framework on other javascript browser games.
3. Connect the game with other learning algorithms from, for example, [TensorForce](https://github.com/reinforceio/tensorforce).
4. In all the models I've trained so far the dino prefers jumping over the birds rather than ducking underneath them. Eventhough I've modified the reward function to encourage the dino to duck instead of jump. It would be interesting to see which reward function or approach  makes the dino duck.
5. ofcourse: improve the performance :) -->
</section>
</section>
<section id="references" class="level2">
<h2 class="anchored" data-anchor-id="references">References</h2>
<p>[1] <a href="https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf">Playing Atari with Deep Reinforcement Learning</a> <br> [2] <a href="http://cs229.stanford.edu/proj2016/report/KeZhaoWei-AIForChromeOfflineDinosaurGame-report.pdf">AI for Chrome Offline Game</a> <br> [3] <a href="https://github.com/ivanseidel/IAMDinosaur">IAMDinosaur</a> <br> [4] <a href="https://en.wikipedia.org/wiki/Reinforcement_learning">Wikipedia page RL</a> <br> [5] <a href="https://arxiv.org/pdf/1606.01540.pdf">Open AI Gym</a> <br> [6] <a href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Resources_files/deep_rl.pdf">RL course David Silver</a> <br> [7] <a href="https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-4-deep-q-networks-and-beyond-8438a3e2b8df">RL blogpost</a> <br> [8] <a href="https://arxiv.org/pdf/1509.02971.pdf">Target network paper</a> <br> [9] <a href="https://arxiv.org/abs/1511.05952">Prioritized Experience Replay</a></p>
<section id="thanks" class="level3">
<h3 class="anchored" data-anchor-id="thanks">Thanks!</h3>
<p>Thanks for reading this lengthy post about my project. Hope you enjoyed it and found it useful in some way. If you have any question, please don’t hesitate to drop me an <a href="mailto:dutordoirv@gmail.com">email</a> or find me on <a href="https://twitter.com/vdutor">twitter</a>.</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
      <div class="nav-footer-center">Last updated on 19 Jun 2023</div>
  </div>
</footer>



</body></html>